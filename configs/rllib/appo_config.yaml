# configs/rllib/appo_config.yaml

algorithm:
  # --- Instruct RLlib to use the OLDER API stack for model config ---
  # This allows using the 'model:' key directly as you specified.
  _enable_rl_module_api: false
  _enable_connector_api: false
  # OR, some versions might prefer this for disabling the new stack:
  # api_stack:
  #   enable_rl_module_and_learner: false
  #   enable_env_runner_and_connector_v2: false

  # General Training Parameters
  framework: "torch"
  num_gpus: 0 # Set to 1 or more if you have GPUs

  # Environment Configuration
  env: "GreenDCC_RLlib_Env" # Must match the name registered in your script
  env_config:
    sim_config_path: "configs/env/sim_config.yaml"
    dc_config_path: "configs/env/datacenters.yaml"
    reward_config_path: "configs/env/reward_config.yaml"
    # Ensure sim_config.yaml has single_action_mode: true

  # Rollout Worker Configuration
  num_env_runners: 16 # APPO benefits from many runners
  num_cpus_per_env_runner: 1
  # rollout_fragment_length: 50 # APPO default; number of steps collected per worker per sample
                              # Total samples per iteration = num_env_runners * rollout_fragment_length

  # APPO Specific Training Hyperparameters
  lr: 0.0001             # APPO often uses smaller LRs
  gamma: 0.99
  lambda: 1.0            # GAE lambda (1.0 for N-step returns, common with APPO/IMPALA V-trace)
  vf_loss_coeff: 0.5
  entropy_coeff: 0.01
  vtrace: true           # Use V-trace correction (key for APPO's off-policy nature)
  vtrace_clip_rho_threshold: 1.0
  vtrace_clip_pg_rho_threshold: 1.0
  # num_sgd_iter: 1      # APPO typically does fewer SGD passes per batch of data (often 1)
                         # as it's more asynchronous and learners pull data continuously.
  train_batch_size: 512  # Size of data processed by ONE learner SGD step.
                         # Total data processed per "training iteration" reported by Tune
                         # is more complex due to asynchronous nature but related to this.
                         # It's the batch size for each SGD step on the learner.
  learner_queue_size: 16 # Size of queue for data from workers to learner
  learner_queue_timeout: 300
  # min_time_s_per_iteration: 10 # Optional: Ensure iterations take at least this long

  # Model Configuration (OLD API STACK STYLE)
  model:
    fcnet_hiddens: [128, 128]
    fcnet_activation: "relu"
    # vf_share_layers: true # APPO/IMPALA often default to shared layers or handle it differently

  # Evaluation
  evaluation_interval: 25
  evaluation_duration: 5
  evaluation_duration_unit: "episodes"
  evaluation_num_env_runners: 1 # Can use evaluation_num_workers in older versions
  evaluation_config:
    explore: false
    env_config:
      sim_config_path: "configs/env/sim_config.yaml"
      dc_config_path: "configs/env/datacenters.yaml"
      reward_config_path: "configs/env/reward_config.yaml"

# --- Tune/AIR Run Configuration ---
run_config:
  name: "GreenDCC_APPO_SingleAction"
  stop:
    timesteps_total: 10000000 # Stop after N environment steps
    # training_iteration: 5000 # Alternative stopping condition
  checkpoint_config:
    num_to_keep: 5
    checkpoint_frequency: 100 # Save checkpoint every N training iterations
    checkpoint_score_attribute: "evaluation/env_runners/episode_return_mean" # Verify this exact key
    checkpoint_score_order: "max"
    checkpoint_at_end: true
  # verbose: 1