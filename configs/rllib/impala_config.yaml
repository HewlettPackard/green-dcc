# configs/rllib/impala_config.yaml

algorithm:
  # --- Instruct RLlib to use the OLDER API stack for model config ---
  _enable_rl_module_api: false
  _enable_connector_api: false
  # OR, some versions might prefer this for disabling the new stack:
  # api_stack:
  #   enable_rl_module_and_learner: false
  #   enable_env_runner_and_connector_v2: false

  # General Training Parameters
  framework: "torch"
  num_gpus: 0 # Set to 1 or more if using GPUs for the learner

  # Environment Configuration
  env: "GreenDCC_RLlib_Env" # Must match the name registered in your script
  env_config:
    sim_config_path: "configs/env/sim_config.yaml"
    dc_config_path: "configs/env/datacenters.yaml"
    reward_config_path: "configs/env/reward_config.yaml"
    # Ensure sim_config.yaml has single_action_mode: true

  # Rollout Worker Configuration (IMPALA uses 'num_workers' which implies EnvRunners)
  num_workers: 16 # Number of rollout workers (actors)
  num_cpus_per_env_runner: 1 # CPUs for each rollout worker
  rollout_fragment_length: 128 # IMPALA's default, steps per worker per sample batch to learner

  # IMPALA Specific Training Hyperparameters
  lr: 0.0005             # IMPALA can sometimes use slightly higher LRs than APPO
  gamma: 0.99
  lambda: 1.0            # GAE lambda (1.0 for V-trace, as IMPALA uses V-trace)
  vf_loss_coeff: 0.5
  entropy_coeff: 0.01
  vtrace: true           # Crucial for IMPALA
  vtrace_clip_rho_threshold: 1.0
  vtrace_clip_pg_rho_threshold: 1.0
  # num_sgd_iter: 1      # IMPALA learners process data as it comes; typically 1 pass
  train_batch_size: 1024  # Batch size for SGD updates on the learner(s)
                         # Total data throughput depends on num_workers, rollout_fragment_length, and learner speed
  num_data_loader_buffers: 1 # Default for IMPALA
  min_time_s_per_iteration: 10 # Can help stabilize reporting if iterations are very fast
  
  # Learner configuration (IMPALA can have multiple learners)
  num_learners: 0 # 0 means learner runs on driver. 1 or more for dedicated learner actors.
  num_gpus_per_learner: 0 # If num_learner_workers > 0 and you want them on GPUs

  # Model Configuration (OLD API STACK STYLE)
  model:
    fcnet_hiddens: [128, 128]
    fcnet_activation: "relu"
    # vf_share_layers: true # IMPALA often defaults to shared layers.

  # Evaluation
  evaluation_interval: 25
  evaluation_duration: 5
  evaluation_duration_unit: "episodes"
  evaluation_num_workers: 1 # For evaluation EnvRunners
  evaluation_config:
    explore: false
    env_config:
      sim_config_path: "configs/env/sim_config.yaml"
      dc_config_path: "configs/env/datacenters.yaml"
      reward_config_path: "configs/env/reward_config.yaml"

# --- Tune/AIR Run Configuration ---
run_config:
  name: "GreenDCC_IMPALA_SingleAction"
  stop:
    timesteps_total: 10000000
  checkpoint_config:
    num_to_keep: 5
    checkpoint_frequency: 100
    checkpoint_score_attribute: "evaluation/episode_reward_mean" # Using common eval metric. VERIFY THIS KEY.
    checkpoint_score_order: "max"
    checkpoint_at_end: true
  # verbose: 1