algorithm:
  name: SAC
  gamma: 0.99
  alpha: 0.01
  actor_learning_rate: 1e-4
  critic_learning_rate: 3e-4
  batch_size: 512
  tau: 0.005
  replay_buffer_size: 100000
  warmup_steps: 1000
  total_steps: 10000000
  update_frequency: 1
  policy_update_frequency: 2
  save_interval: 5000
  log_interval: 100
  hidden_dim: 64
  max_tasks: 750
  device: auto  # auto = use CUDA if available

  # Optional: Attention-specific hyperparameters (if use_attention: true)
  use_attention: false
  attention:
    embed_dim: 64
    num_heads: 1
    num_attention_layers: 1
    hidden_dim_ff: 64 # dim_feedforward in TransformerEncoderLayer
    dropout: 0.1